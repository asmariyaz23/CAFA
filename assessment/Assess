#!/usr/bin/env python

import os
import sys
import re
import argparse
from collections import defaultdict
from matplotlib import pyplot as py
import ArgParser
import Propagate
import FilterPredictionSet
import CreateDataset
import Compare
import Config
import ConfigParser
from os.path import basename
import shutil

'''
   The CAFA Assessment tool is used for comparing user predictions with CAFA benchmark
   There are 2 types of assessment performed : protein centric and term centric.
   This program currently only assess using protein centric measure.

   The assessment program takes in 2 files: One file containing predictions 
   for a set of target proteins and the other file containing the true 
   experimental validations for the same targets

   Output : A text file containing precision and recall values at all thresholds
   provided by the user and a precision-recall curve of the same
'''




print "Welcome to the CAFA Assessment Tool!!!!"
print "***************************************\n"

parser = argparse.ArgumentParser(prog='Assess',description='Asseess the predictions produced through different prediction methods')

parser.add_argument('-P', '--prediction',action='store',nargs='*',help='Specifies the path to a predictions file.')
parser.add_argument('-B', '--benchmark',action='store' ,nargs='*',help='Specifies the path to a benchmark file.')
parser.add_argument('-O', '--output', nargs='*',default=[], help='Optional argument.Provides user with option to specify output filename.')
parser.add_argument('-N', '--ontology',action='store',nargs='*',default=['all'],help='Provides user a choice to specify a list of ontologies(example: F, P, C) separated by space.Default is all.')
parser.add_argument('-R', '--prop',action='store',default='F',help='Can take in either T of F values. If T, the program will assume that the predictions and benchmark are already propagated. Default is F.')

# Search for config file in the current directory and if missing, creates a new one                                                                                             
fname_ind = 0

for root,dirs,files in os.walk('.'):
    for fname in files:
        if fname == '.cafarc':
            fname_ind = 1
    if fname_ind == 0:
        print 'Config file not found'
        print 'Creating new configuration file...'
        print '***************************************************************'
        Config.create()
    break

Config_handle = ConfigParser.ConfigParser()
Config_handle.read('.cafarc')
ConfigParam = defaultdict()

ConfigParam = {'workdir' : Config_handle.get('WORKDIR', 'DEFAULT_PATH')
               }

work_dir = ConfigParam['workdir']
work_dir = work_dir.rstrip('/')

# If working directory is already present, creates one 
if not os.path.exists(work_dir):
    os.makedirs(work_dir)

parsed_input = ArgParser.parse(parser)
print "Successfully validated inputs.......\n"

# Check if predictions file exists

pred_file_basename = basename(parsed_input['pred_file'])
if os.path.exists(work_dir + '/' + pred_file_basename):
    pred_file = work_dir + '/' + pred_file_basename
elif os.path.exists(parsed_input['pred_file']):
    shutil.move(parsed_input['pred_file'],work_dir)
    pred_file = work_dir + '/' + pred_file_basename
else:
    print parsed_input['pred_file'] + ' is not available.'
    sys.exit(1)

# Check if benchmark file exists

bench_file_basename = basename(parsed_input['bench_file'])
if os.path.exists(work_dir + '/' + bench_file_basename):
    bench_file = work_dir + '/' + bench_file_basename
elif os.path.exists(parsed_input['bench_file']):
    shutil.move(parsed_input['bench_file'],work_dir)
    bench_file = work_dir + '/' + bench_file_basename
else:
    print parsed_input['bench_file'] + ' is not available.'
    sys.exit(1)

# Format output filename based on supplied output prefix

outfile_basename = basename(parsed_input['outfile'])

if not outfile_basename == '':
    ob = work_dir + '/' + outfile_basename
else:
    ob = pred_file + '.prec_rec'

# Check format of prediction file

pred_file_handle = open(pred_file, 'r')
print "Checking prediction file format.........\n"

for lines in pred_file_handle:
    fields = lines.strip().split()
    if re.match('^AUTHOR', lines):
        print 'These predictions have been submitted by : ' + fields[1]
        continue
    if re.match('^MODEL', lines):
        if not fields[1] == '1':
            print 'Submitted predictions for model : ' + fields[1] + ' Please check and resubmit again if you wish to.'
        else:
            print 'Submitted predictions for model : ' + fields[1]
        continue

    if re.match('^KEYWORDS', lines):
        print 'Keywords mentioned by the author are the following : ' + fields[1]
        continue
    if re.match('^ACCURACY\s{1}',lines):
        if len(fields) < 3:
            print 'Incorrect format for entering accuracy values. Please check and resubmit again.'
        else:
            method_prec = fields[1].replace(';','')
            method_rec = fields[2].replace(' ', '')
            print 'Precision for the method : ' + method_prec
            print 'Recall for the method : ' + method_rec
        continue
    if re.match('^END', lines):
        continue
    if lines == ' ':
        continue
    if not len(fields) == 3:
        print 'Incorrect file format.Please check and submit again.\n'
        sys.exit(1)
    if not re.match('GO:\d+',fields[1]) and re.match('\d\.\d+',fields[2]):
        print 'The second column should be the GO term and last column should be threshold.\n'
        sys.exit(1)

print 'Successfully validated prediction file format\n'

# Check format of benchmark file

exp_file_handle = open(bench_file, 'r')
print "Checking benchmark file format.......\n"

for line in exp_file_handle:
    fields = line.strip().split()

    if len(fields) < 2:
        print 'You file has missing data. Please check and submit again.\n'
        sys.exit(1)
    if not re.match('GO:\d+',fields[1]) :
        print 'Problems with values in some fields.Please check and submit again.\n'
        sys.exit(1)
   
print 'Successfully validated benchmark file format.\n'

# Check for Propagation status. If predictions are not propagated, propagates them

if parsed_input['prop_status'] == 'T':
    propagated_pred_file = pred_file
    propagated_bench_file = bench_file
    print "Prediction and bench data are already propagated."
    final_pred_set, final_bench_set = FilterPredictionSet.pred_filter(propagated_pred_file, propagated_bench_file)
else:
    propagated_pred_file = Propagate.propagate_prediction(pred_file)
    propagated_bench_file = Propagate.propagate_benchmark(bench_file)
    final_pred_set, final_bench_set = FilterPredictionSet.pred_filter(propagated_pred_file, propagated_bench_file)

# After the predictions and benchmark set are propagated and we have extracted the pred and bench set to actually assess, the next step is to check the user's choice of ontology for doing the assessment 

user_ontology = parsed_input['user_ontology']
pred_ann_mfo, pred_ann_bpo, pred_ann_cco = pred_annotation_mfo = CreateDataset.createPred(final_pred_set, user_ontology)
true_ann_mfo, true_ann_bpo, true_ann_cco, unique_prots_mfo, unique_prots_bpo, unique_prots_cco = CreateDataset.createBench(final_bench_set, user_ontology)

if pred_ann_mfo and true_ann_mfo:
    print "Assessing predictions in the molecular function category......"
    Compare.calc(pred_ann_mfo, true_ann_mfo, unique_prots_mfo, ob, 'F')
if pred_ann_bpo and true_ann_bpo:
    print "Assessing predictions in the biological process category......"
    Compare.calc(pred_ann_bpo, true_ann_bpo, unique_prots_bpo, ob, 'P')
if pred_ann_cco and true_ann_cco:
    print "Assessing predictions in the cellular components category......"
    Compare.calc(pred_ann_cco, true_ann_cco, unique_prots_cco, ob, 'C')

# Clean working directory

if parsed_input['prop_status'] == 'F':
    if os.path.exists(propagated_pred_file):
        os.remove(propagated_pred_file)
    if os.path.exists(propagated_bench_file):
        os.remove(propagated_bench_file)

if os.path.exists(final_pred_set):
    os.remove(final_pred_set)
if os.path.exists(final_bench_set):
    os.remove(final_bench_set)
