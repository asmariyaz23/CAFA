#!/usr/bin/python

# The assessment program takes in 2 files: One file containing predictions for a set of target proteins and the other file containing the true experimental validations for the same targets

import os
import sys
import re
import argparse
from collections import defaultdict
from matplotlib import pyplot as py
import ArgParser
import Propagate
import FilterPredictionSet
import CreateDataset
import Compare


print "Welcome to the CAFA Assessment Tool!!!!"
print "*************************************************\n"

parser = argparse.ArgumentParser(prog='Assess',description='Assess predictions from different prediction methods')

parser.add_argument('-P', '--prediction',action='store',nargs='*',help='Specifies the path to a file containing predictions from a specific method. The file needs to follow the required format to be accepted by the software.')
parser.add_argument('-B', '--benchmark',action='store' ,nargs='*',help='Specifies the path to a file containing true experimental validations for the target proteins. This can be a simple tab delimited file with 2 columns, the first column is the protein identifier and second column contains the GO term.')
parser.add_argument('-N', '--ontology',action='store',nargs='*',default=['all'],help='Provide a choice as to what type of ontology would the user like to assess. In the default event, assessment of all ontologies will be provided')
parser.add_argument('-R', '--prop',action='store',default='F',help='Mention whether the input prediction file has predictions propagated to the root or not. By default, it is assumed that the predictions are unpropagated.Set to False')

parsed_input = ArgParser.parse(parser)
print "Successfully validated inputs.......\n"

# Check file formats

pred_file_handle = open(parsed_input['pred_file'], 'r')
print "Checking prediction file format"

for lines in pred_file_handle:
    fields = lines.strip().split()
    if re.match('^AUTHOR', lines):
        print 'These predictions have been submitted by : ' + fields[1]
        continue
    if re.match('^MODEL', lines):
        if not fields[1] == '1':
            print 'Submitted predictions for model : ' + fields[1] + ' Please check and resubmit again if you wish to.'
        else:
            print 'Submitted predictions for model : ' + fields[1]
        continue

    if re.match('^KEYWORDS', lines):
        print 'Keywords mentioned by the author are the following : ' + fields[1]
        continue
    if re.match('^ACCURACY\s{1}',lines):
        if len(fields) < 3:
            print 'Incorrect format for entering accuracy values. Please check and resubmit again.'
        else:
            method_prec = fields[1].replace(';','')
            method_rec = fields[2].replace(' ', '')
            print 'Precision for the method : ' + method_prec
            print 'Recall for the method : ' + method_rec
        continue
    if re.match('^END', lines):
        continue
    if lines == ' ':
        continue
    if not len(fields) == 3:
        print lines
        print 'Incorrect file format.Please check and submit again.\n'
        sys.exit(1)
    if not re.match('GO:\d+',fields[1]) and re.match('\d\.\d+',fields[2]):
        print 'The second column should be the GO term and last column should be threshold.\n'
        sys.exit(1)

print 'Successfully validated prediction file format'

exp_file_handle = open(parsed_input['bench_file'], 'r')
print "Checking benchmark file format."

for line in exp_file_handle:
    fields = line.strip().split()

    if len(fields) < 2:
        print 'You file has missing data. Please check and submit again.\n'
        sys.exit(1)
    if not re.match('GO:\d+',fields[1]) :
        print 'Problems with values in some fields.Please check and submit again.\n'
        sys.exit(1)
   
print 'Successfully validated benchmark file format.\n'

# Check for Propagation status

if parsed_input['prop_status'] == 'T':
    propagated_pred_file = parsed_input['pred_file']
    propagated_bench_file = parsed_input['bench_file']
    print "Prediction and bench data are already propagated."
    final_pred_set, final_bench_set = FilterPredictionSet.pred_filter(propagated_pred_file, propagated_bench_file)
else:
    propagated_pred_file = Propagate.propagate_prediction(parsed_input['pred_file'])
    propagated_bench_file = Propagate.propagate_benchmark(parsed_input['bench_file'])
    final_pred_set, final_bench_set = FilterPredictionSet.pred_filter(propagated_pred_file, propagated_bench_file)

# After the predictions and benchmark set are propagated and we have extracted the pred and bench set to actually assess, the next step is to check the user's choice of ontology for doing the assessment 

user_ontology = parsed_input['user_ontology']
pred_ann_mfo, pred_ann_bpo, pred_ann_cco = pred_annotation_mfo = CreateDataset.createPred(final_pred_set, user_ontology)
true_ann_mfo, true_ann_bpo, true_ann_cco, unique_prots_mfo, unique_prots_bpo, unique_prots_cco = CreateDataset.createBench(final_bench_set, user_ontology)

if pred_ann_mfo and true_ann_mfo:
    print "Assessing predictions in the molecular function category......"
    Compare.calc(pred_ann_mfo, true_ann_mfo, unique_prots_mfo, parsed_input['pred_file'], 'F')
if pred_ann_bpo and true_ann_bpo:
    print "Assessing predictions in the biological process category......"
    Compare.calc(pred_ann_bpo, true_ann_bpo, unique_prots_bpo, parsed_input['pred_file'], 'P')
if pred_ann_cco and true_ann_cco:
    print "Assessing predictions in the cellular components category......"
    Compare.calc(pred_ann_cco, true_ann_cco, unique_prots_cco, parsed_input['pred_file'], 'C')

